

\chapter{Methodology}

\section{Overview}
This chapter outlines the methodology employed for image processing and evaluating the quality of the lens. The process involves several steps, from capturing and preprocessing images to analyzing various lens properties. The chapter also describes the architecture of the system, data flow among the system and features a set of scenarios for simple lens evaluation. This chapter concludes with the design of User Interface Design.

\section{Unconventional Testing Environment}
While professional lens testing typically requires expensive equipment and controlled lighting, this methodology embraces a more accessible "garage lab" approach. Basic household items are repurposed for testing - a white wall becomes the vignetting test surface, while Christmas lights or phone flashlights serve as point light sources for bokeh analysis. This approach, while less precise than industrial solutions, provides meaningful relative measurements that help photographers evaluate their lenses.

\subsection{Calibration Elements}
\subsubsection{DIY Test Charts}
Rather than requiring expensive calibration targets, the methodology relies on printable test charts that users can create at home. A simple grid printed on an office printer serves as a distortion target, while a Siemens star pattern tests resolution. The key is consistency in printing and mounting - charts should be flat and well-lit, even if the absolute precision is lower than commercial targets.

\subsection{Testing Scenarios}

\subsubsection{The White Wall Technique} %todo prepisat
The vignetting analysis relies on photographing a uniformly lit white wall. While professional testing uses integrating spheres, a well-lit white wall provides sufficient data for relative comparisons. Users are instructed to position the camera square to the wall and ensure even illumination through test shots.

\subsubsection{Bokeh Analysis Setup} 
Point light sources for bokeh testing can be created using simple LED lights or phone flashlights in a darkened room. The methodology emphasizes consistent testing distances and background separation rather than absolute measurements. This allows meaningful comparisons between lenses even in improvised setups.

\subsection{Environmental Considerations}

\subsubsection{Lighting Compensation}
Rather than requiring expensive studio lighting, the methodology adapts to available light sources. Test procedures include steps for compensating for uneven ambient lighting through reference shots and software correction. This makes testing possible in various environments while maintaining relative accuracy.

\subsection{Consistency Through Process} %todo prepisat
While individual measurements may have higher variability than lab tests, the methodology emphasizes consistency through careful documentation and repeated measurements. By averaging multiple tests and following structured procedures, hobbyist photographers can build meaningful datasets about their lens performance using accessible tools and spaces.

\section{Image Acquisition}

\subsection{Camera Control System}
The image acquisition system uses gphoto2 to manage camera control effectively. It provides automated detection and configuration of cameras, simplifying the setup process. The system also facilitates the management of exposure and focus parameters. Robust error handling and recovery procedures are implemented to ensure uninterrupted operation. Additionally, the system is optimized for capturing images in RAW format, preserving maximum image quality for detailed analysis.

\subsection{Camera Setup and Configuration}
The system implements a structured approach to camera management by incorporating key components that ensure reliability and accuracy. Setting validation is a critical aspect, including parameter range checking to confirm that values fall within acceptable limits, configuration compatibility verification to prevent conflicts, and setting application confirmation to ensure that changes are successfully applied. Additionally, error state recovery mechanisms are in place to handle unexpected issues and maintain smooth operation.

\subsection{RAW Image Capture}
The image acquisition process follows to a defined protocol that ensures reliability and efficiency.

\subsubsection{Pre-capture Steps}
Pre-capture steps include verifying camera readiness with timeout handling to avoid delays, validating configuration parameters to ensure compatibility, allocating memory buffers for seamless data handling, and confirming the capture target to direct where the image will be stored. These measures collectively provide a robust foundation for the image capture process.

\subsubsection{Capture Process}

The capture process involves a sequence of well-defined steps. If required, the process begins with mirror lock-up to minimize vibrations during the capture. This is followed by the execution of the capture command and the initiation of file transfer from the camera. Data integrity is then verified to ensure the accuracy of the captured image, and finally, local storage management is performed to organize and store the data efficiently.

% TODO: neviem ci pouzit
\begin{enumerate}
    \item Mirror lock-up
    \item Capture command execution
    \item File transfer initiation
    \item Data integrity verification
    \item Local storage management
\end{enumerate}

\subsection{Batch Capture Implementation}
The batch capture implementation is designed to facilitate scenarios that require multiple captures with different settings.

\subsubsection{Batch Process Control}
 Batch process control ensures a seamless workflow by defining the sequence of parameters for each capture and automatically adjusting settings between captures. Additionally, progress monitoring and status reporting provide real-time updates on the batch process, while robust error-handling mechanisms allow the process to continue uninterrupted, even in the presence of individual capture failures.

\subsubsection{Data Management}
Data management plays a critical role in the batch capture process. A structured file naming convention is employed to maintain consistency and ease of identification, while metadata is embedded directly into the captured files for enhanced traceability. Temporary storage is managed efficiently to optimize space usage during the batch process, and the final dataset is organized systematically to ensure accessibility and compatibility with subsequent analysis workflows.

\subsection{Focus Control System}

The system implements both automatic and manual focus control. Autofocus can be engaged or disengaged as needed. Manual focus drive control offers fine-tuned adjustments for scenarios requiring greater precision. Focus confirmation ensures that the desired focus point has been achieved. The system tracks focus distance in real-time, providing valuable data for maintaining accuracy and consistency across captures.

\subsection{Error Handling and Recovery}

\subsubsection{Error Categories}
The system implements robust error handling mechanisms at multiple levels:

\begin{itemize}
    \item Connection failures
    \item Configuration errors
    \item Capture failures
    \item File transfer issues
\end{itemize}

Each error category is addressed with targeted strategies to minimize disruption and maintain smooth operation.

\subsubsection{Recovery Mechanisms}
Recovery mechanisms are an integral part of the systemâ€™s error management framework. Automatic reconnection attempts are employed to restore communication in the event of connection failures. Configuration reset capabilities allow the system to recover from misconfigurations efficiently. Capture retry logic ensures that transient issues during image capture do not halt the overall process. Resource cleanup procedures are implemented to prevent system clutter and ensure that resources are appropriately managed following errors.

\section{Software Architecture Decisions}

\subsection{Dataset Management Architecture}
The dataset management architecture implements a hierarchical file-based organization system designed to maintain relationships between captured images, their analysis results, and associated metadata. Each dataset represents a complete lens evaluation session, structured as a self-contained directory with standardized subdirectories for scenarios, images, and analysis outputs.

At the root of each dataset, a JSON-based manifest file maintains dataset metadata and provides an index of all contained scenarios and images. This manifest approach enables quick dataset listing and scenario selection without requiring full directory traversal or image loading. The architecture separates the storage of large image files from their metadata, allowing efficient loading of analysis results without immediately incurring the memory overhead of image data.

Scenarios within a dataset maintain their own subdirectories and metadata files, creating logical groupings of related captures. Each scenario tracks its type (e.g., vignetting, bokeh, distortion), creation timestamp, and a list of associated captures with their camera settings. This organization facilitates both single-scenario analysis and comparative evaluation across different testing conditions.

The architecture implements an import/export system that packages datasets into portable ZIP archives while maintaining internal reference integrity. This enables dataset sharing and backup while preserving the relationship between captures and their analysis results. The system employs temporary directories during import operations to ensure atomic dataset creation, preventing partial or corrupted imports from affecting existing data.

Storage efficiency is considered through selective JPEG preview generation, with computationally intensive RAW processing performed only when required for analysis. The architecture maintains relative paths within datasets, enabling relocation of dataset directories without breaking internal references.

\subsection{Camera Control Architecture}
The camera control architecture implements a state machine pattern to manage camera interactions and maintain connection stability. At its core, a camera manager class encapsulates all camera operations behind a thread-safe interface, utilizing a locking mechanism to prevent concurrent access conflicts during critical operations.

The architecture employs a background monitoring system that periodically verifies camera connectivity and readiness. This monitoring runs in a separate daemon thread, checking camera status every two seconds and attempting automatic recovery when connection issues are detected. The system manages three primary states: disconnected, connected-but-busy, and ready-for-capture.

For camera configuration management, the architecture implements a hierarchical settings structure that mirrors the camera's internal configuration tree. This approach allows for granular control over camera parameters while maintaining compatibility with different camera models that may expose varying levels of configurability. The system caches frequently accessed settings to reduce I/O overhead during repeated operations.

Error handling is implemented through a layered approach, with low-level camera errors being caught and translated into application-specific exceptions. The architecture includes timeout mechanisms for camera operations, preventing indefinite waits during hardware communication issues. Recovery procedures are automated where possible, with graceful degradation when full recovery cannot be achieved.

Memory management considerations are addressed through careful handling of image data buffers, particularly during RAW capture operations where memory requirements are substantial. The architecture implements immediate cleanup of temporary buffers and provides mechanisms for streaming large images directly to storage rather than holding them in memory.

While primarily tested with Canon EOS models, the architecture attempts to maintain vendor neutrality by accessing camera features through generic gphoto2 interfaces rather than relying on vendor-specific commands where possible. This design decision trades some advanced feature access for broader compatibility.
\subsection{State Management}
The system employs a thread-safe state machine to maintain camera states throughout operation cycles. The connection state continuously monitors camera connectivity and implements automatic recovery mechanisms when needed. An operation state handler manages active camera operations including capture and configuration processes. To prevent race conditions and ensure data integrity, a lock state mechanism provides thread-safe access to all camera resources.

\subsection{Error Recovery System}
The error handling system provides comprehensive recovery mechanisms for various failure scenarios. When connection failures occur, the system automatically initiates reconnection attempts with exponential backoff. Temporary errors are handled through operation retry logic, while hardware issues trigger graceful degradation procedures to maintain basic functionality. Critical failures activate resource cleanup protocols to prevent system resource leaks.

\subsection{Event System}
Asynchronous camera operations are managed through a sophisticated event system. This system broadcasts status changes to all relevant components, handles operation completion notifications, and provides detailed context for error events. The event system maintains a consistent view of camera state across all system components through connection state broadcasts.

\subsection{Analysis Results Architecture}
The architecture for storing and managing analysis results follows a hierarchical structure designed to maintain organization across different types of lens evaluations. Each analysis result is stored as a dictionary containing both numerical measurements and visualization data, with standardized fields for consistent access and comparison.

The core results structure includes metadata about the capture conditions, raw measurements, processed metrics, and paths to generated visualizations. For vignetting analysis, the results store center-to-corner intensity ratios along with a heat map visualization. Sharpness analysis results maintain edge intensity measurements, MTF values, and comparative visualizations showing detected edges against the original image.

To handle different analysis types consistently, the architecture implements a standardization layer that converts raw measurements into calibrated scores between 0 and 100. This facilitates meaningful comparisons between different lenses while preserving the underlying raw data for detailed examination.

The architecture also accounts for result persistence, storing analysis outputs alongside their corresponding RAW captures in a structured filesystem hierarchy. This organization enables both immediate result display and later retrieval for comparative analysis, while keeping the storage requirements manageable by separating the analysis results from the much larger RAW image files.

An important consideration in the design was error handling - results objects include fields for capturing analysis failures and partial results, rather than requiring all measurements to succeed for storing an analysis output. This pragmatic approach acknowledges the challenges of real-world lens testing while maintaining result integrity.

While not attempting to match the sophistication of commercial solutions, this architecture provides sufficient structure to support meaningful lens evaluation within the scope of an educational tool.

\subsection{Storage Format}
The system utilizes a hierarchical JSON-based structure for data organization, ensuring both flexibility and accessibility. Each measurement is associated with detailed metadata that captures test conditions and parameters. The system preserves raw data alongside processed results, enabling future reanalysis as algorithms improve. Version control of stored results maintains data integrity and enables tracking of analysis improvements over time.

\subsection{Comparison Framework}
A sophisticated comparison framework enables detailed analysis across different lenses and testing conditions. The system supports both direct cross-lens comparisons and historical trend analysis, incorporating statistical significance testing to validate results. A normalized scoring system accounts for variations in test conditions, ensuring meaningful comparisons across different testing scenarios.

\subsection{Export Capabilities}
The system provides flexible data export options to support various analysis needs. Detailed CSV reports capture numerical data for statistical analysis, while visual comparison charts offer intuitive result interpretation. Raw measurement data export enables external analysis and verification, and comprehensive metadata export ensures full reproducibility of all test results.

\section{Integration Points} %todo subsection or section
The integration points between system components follow a modular design that emphasizes loose coupling and high cohesion. The camera control system communicates with the analysis engine through a message queue system, allowing asynchronous processing of captured images. This design choice reduces system blocking during long-running operations and improves overall responsiveness.

The analysis engine interfaces with the data management system through a unified API that handles both RAW image storage and analysis results. This integration point implements retry logic and transaction management to ensure data consistency, particularly important when handling large RAW files during batch operations.

The web interface connects to the backend systems via a REST-like protocol implemented using the NiceGUI framework. This architecture decision simplifies state management and provides a clear separation between the UI layer and core analysis functions. The interface adapts to different analysis scenarios while maintaining consistent error handling and progress reporting.

\subsection{Performance Metrics}
System performance evaluation focuses on three key areas: image processing speed, analysis accuracy, and resource utilization. The image processing pipeline achieves analysis times of under 5 seconds for standard lens evaluation tasks on commodity hardware. This performance level maintains interactive responsiveness while ensuring thorough analysis of lens characteristics.

Memory management posed a particular challenge when handling RAW image files. The implementation uses streaming processing where possible, only loading complete images into memory when required for analysis. This approach allows the system to process high-resolution images while maintaining a reasonable memory footprint of approximately 2GB during normal operation.

Storage requirements vary based on usage patterns, with a typical analysis session generating between 100MB and 1GB of data including RAW images and analysis results. The system implements automatic cleanup of temporary files while preserving essential analysis data for future comparison.

\subsection{Validation Procedures} 
The validation process implements both automated and manual verification steps to ensure measurement accuracy. Each lens analysis includes control measurements using calibration patterns to validate the core metrics. These controls provide baseline measurements that help identify any systematic errors in the analysis pipeline.

Cross-validation with existing analysis tools provided reference points for accuracy assessment. While the system cannot match the precision of industrial testing equipment, it achieves sufficient accuracy for practical lens evaluation. The validation process identified areas where measurement accuracy could be improved, particularly in chromatic aberration detection under varying lighting conditions.
%todo dopisat
\section{Image Processing Pipeline}

\subsection{Preprocessing}
Image filtering is a crucial step in image preprocessing; its goal is to enhance the image quality and prepare it for subsequent analysis. Filtering helps in reducing noise, smoothing the image, and highlighting important features.

\textbf{Gaussian blur} is a widely-used smoothing technique that applies a Gaussian function to the image. It effectively reduces noise and detail, providing a smoother and more uniform image. The Gaussian blur is particularly useful in preparing images for feature detection and other analytical processes \cite{gaussian}.

\subsection{Feature Detection}
Feature detection on the captured image is performed using the \textbf{Scale-Invariant Feature Transform} (SIFT). This algorithm detects and describes local features in images, also referred to as keypoints in the following sections. SIFT is used in computer vision for object recognition, image stitching, and other applications due to its robustness to changes in scale, rotation, and illumination. In this case it is used for analyzing sharpness, PSF, and bokeh \cite{Sift}.

\subsection{Analysis Algorithms}
The analysis engine implements several specialized algorithms for evaluating lens characteristics, each designed to measure specific aspects of lens performance.

\subsubsection{Vignetting Analysis}
The vignetting analysis algorithm evaluates light falloff from center to edges. It begins by dividing the image into center and peripheral regions, then calculates intensity ratios between these areas. The algorithm compensates for natural light falloff using calibrated correction factors, ultimately producing normalized vignetting scores that accurately represent lens performance.

\subsubsection{Chromatic Aberration Detection}
Chromatic aberration detection focuses on color fringing artifacts in images. The algorithm analyzes RGB channel alignment at high-contrast edges, measuring both the magnitude and direction of color fringing. It distinguishes between lateral and longitudinal chromatic aberration, generating detailed heat maps that visualize aberration distribution across the frame.

\subsubsection{MTF Calculation}
The Modulation Transfer Function calculation employs a sophisticated slanted-edge analysis approach. This process begins with precise edge detection and proceeds through several stages: calculating the edge spread function (ESF), deriving the line spread function (LSF), and finally computing the MTF through Fourier transform. This provides accurate measurements of lens resolution and contrast performance.

%todo

\section{System Architecture}

\subsection{Component Overview}
The system architecture of the lens evaluation application is composed of four main components: image acquisition, segmentation and preprocessing, lens property evaluation, and result display. These components work together to capture images from a camera, process and analyze these images, and present the results to the user through a web interface.

\subsection{Data Flow}
The data flow in the application follows a structured process from image acquisition to result display. Each step is designed to ensure that the captured images are accurately processed and analyzed, and the results are presented in a user-friendly manner.

\subsection{Dataset Concept}
A dataset represents a comprehensive testing session for a specific lens and camera combination. It encapsulates all relevant information about the lens, testing conditions, and captured images. Each dataset receives a unique identifier following the format \texttt{lens\_model\_date}, enabling straightforward tracking and organization.

\subsection{Scenario Design}
Scenarios are structured tests that evaluate specific lens characteristics. Each scenario focuses on isolating a particular optical property through controlled image capture. The primary scenarios include:

Vignetting Analysis: Captures images of a uniform surface at different apertures to measure light falloff.

Bokeh Evaluation: Uses point light sources to assess out-of-focus rendering quality, varying focus and aperture settings.

Distortion Test: Photographs grid patterns to quantify geometric lens deformations across focal lengths.

Sharpness Assessment: Employs resolution test charts to measure lens resolving power and edge performance.

Chromatic Aberration Examination: Analyzes high-contrast edges to detect color fringing and misalignment.

\subsection{Frame Management}
Frames represent individual captured images within a scenario. Each frame stores comprehensive metadata including:

Camera settings (aperture, shutter speed, ISO)
Capture timestamp
Lens configuration
Analysis results specific to the scenario

Frames are named systematically to reflect their testing context: \texttt{scenario\_type\_timestamp\_settings}.

\subsection{User Interface Considerations}
The data organization influences the user interface design, providing intuitive navigation through tests and results. The interface reflects the hierarchical nature of the data, allowing users to drill down from datasets to specific frame results while maintaining context.